Assignment 3 - Replicating a Classic Experiment  
===
Meijie Wang, Yuting Han, Zhiyi Huang

The experiment itself can be seen at this locations: 
https://yutinghan.github.io/03-Experiment

We decide to test human perception of graphic area during this replication of the seminal Cleveland Mcgill experiment, so choose these three different types of charts: Stacked Area Charts, Bubble Charts, and Pie Charts.Each participant was shown 20 variations of each type of chart, each with 10 randomly generated white regions, and two black dots randomly to compare the selected area.

Functions
===
For all the three functions we do, we set the range of the random data, and all difference between values in a set greater than 1, to make sure the readability for all the images.

Stacked Area Charts:
In this chart, set three position dots in each line, the simple random data would leads to line crossing, it's hard to pick two selected areas to compare. So, the data we apply in this chart not untill set the range and difference, but also generated by using overlay method to solve this problem.
In particular, we set the second dot to be the highest in a line, to be more realistic.

![linechart.png](img/linechart.png)
Bubble Charts:
In this chart, we considered that participants may judge the radius of each bubble, a.k.a length instead of the area, may violate our design purpose. So, for the two dots in the bubble we don't set them at the centorid position to avoid that.  
![bubblechart.png](img/bubblechart.png)

Pie Charts:
![PieChart.png](img/PieChart.png)

Set the input should be among 0 to 1, otherwise shows an invail input alert, when finished whole test, show the work done.
![ggplot2](Show1.gif)
# 
![ggplot2](Show2.gif)

Hypotheses
===
1.Participants will make more accurate measurements with the vertical bar chart than the horizontal bar chart.
2.Participants will make more accurate measurements with the horizontal bar chart than the upside down bar chart.
3.Participants will make the least accurate measurements with the upside down bar chart.
4.Participants will make similarly accurate measurements with the horizontal bar chart and pie chart.


Results
===


Technical Achievements
===


Design Achievements
===


Sources
===



Assignment 3 - Replicating a Classic Experiment  
===

For the scope of this project, assume the role of a scientist who runs experiments for a living.

Q: How do we know that bar charts are "better" than pie charts?  
A: Controlled experiments!

In this assignment you'll implement a simple controlled experiment using some of the visualizations you’ve been building in this class. 
You'll need to develop support code for the experiment sequence, results file output, and other experiment components. 
(These are all simple with Javascript buttons and forms.)
The main goals for you are to a) test three competing visualizations, b) implement data generation and error calculation functions from Cleveland and McGill's 1984 paper, c) run the experiment with 10 participants, and d) do some basic analysis and reporting of the results.

For this assignment you should write everything from scratch -- except for charts you've made in previous assignments.
You may *reference* demo programs from books or the web, and if you do please provide a References section with links at the end of your Readme.

Requirements
---

- Look it over Cleveland and McGill's original experiment (see the section below) and [watch this video](experiment-example.mp4) to get a sense of the experiment structure and where your visualizations will go.
- When viewing the example experiment video, note the following:
    - Trials are in random order.  
    - Each trial has a randomly generated set of 10 data points.  
    - Two of these data points are marked.  
- Implement the data generation code **as described in the Cleveland & McGill paper**. 
    - The goal is to generate 5 or 10 random datapoints (see relevant papers for details on generating data, etc) and to mark two of them for comparison in the trial. 
- Add 3 of your (hopefully) existing visualizations to the experiment. When you are adding these visualizations, think about *why* these visualizations are interesting to test. In other words, keep in mind a *testable hypothesis* for each of the added visualization. Some good options include your bar charts, pie charts, stacked-bar charts, and treemaps. You can also rotate your bar chart to be horizontal or upside-down as one of your visualizations. You are encouraged to test unorthodox charts -- radar charts come to mind as something that has never been tested.
    - Follow the style from Cleveland and McGill closely (i.e. no color) unless you are specifically testing a hypothesis (i.e. color versus no color). Pay attention to spacing between bars. Do not mark bars for comparison using color-- this makes the perceptual task too easy.
- After each trial, implement code that grades and stores participant’s responses.
- At the end of the experiment, use Javascript to show the data from the current experiment\* (i.e. a comma separated list in a text box) and copy it into your master datafile. See the Background section below for an example of what this file should look like. (\*Alternately implement a server, if you're experienced with that sort of thing.)

- Figure out how to calculate "Error", the difference between the true percentage and the reported percentage.
- Scale this error using Cleveland and McGill’s log-base-2 error. For details, see the background section (there’s a figure with the equation). This becomes your “Error” column in the output. Make sure you use whole percentages (not decimal) in the log-base-2 equation. Make sure you handle the case of when a person gets the exact percentage correct (log-base-2 of 1/8 is -3, it is better to set this to 0).
- Run your experiment with 10 or more participants.  
    - Grab your friends or people in the class.  
    - Run at least 20 trials per visualization type. If you have 3 visualizations, run at least 60 random trials.
- Make sure to save the resulting CSV after each participant. Compile the results into a master csv file (all participants, all trials).
- Produce a README with figures that shows the visualizations you tested and results, ordered by best performance to worst performance.
- To obtain the ranking, calculate and report the average Error for each visualization across all trials and participants. This should be straightforward to do in a spreadsheet.
- Use Bootstrapped 95\% confidence intervals for your error upper and lower bounds. Include these in your figures. Bootstrapped confidence intervals are easily implemented in R. 
- Include example images of each visualization as they appeared in your experiment (i.e. if you used a pie chart show the actual pie chart you used in the experiment along with the markings, not an example from Google Images).

## General Requirements

0. Your code should be forked from the GitHub repo and linked using GitHub pages.
2. Your project should use d3 to build visualizations. 
3. Your writeup (readme.md in the repo) should contain the following:

- Working link to the experiment hosted on gh-pages.
- Concise description and screenshot of your experiment.
- Description of the technical achievements you attempted with this project.
- Description of the design achievements you attempted with this project.

Background
---

In 1984, William Cleveland and Robert McGill published the results of several controlled experiments that pitted bar charts against pies and stacked-bar variants. 
Their paper (http://www.cs.ubc.ca/~tmm/courses/cpsc533c-04-spr/readings/cleveland.pdf) (http://info.slis.indiana.edu/~katy/S637-S11/cleveland84.pdf) is considered a seminal paper in data visualization.
In particular, they ran a psychology-style experiment where users were shown a series of randomly-generated charts with two graphical elements marked like this:

![cleveland bar chart](img/cleveland-bar.png)

Participants were then asked, "What percentage is the smaller of the larger?". 
This was repeated hundreds of time with varying data and charts. 
By the end of the study, Cleveland and McGill had amassed a large dataset that looked like this:

![cleveland table](img/cleveland-table.png)

__Log-base-2 or "cm-error"__: The true percent is the actual percentage of the smaller to the larger, while the reported percent is what participants reported. 
Cleveland and McGill recognized that their analyses would be biased if they took `abs(ReportedPercent – TruePercent)` as their score for error. 
To compensate, they came up with a logarithmic scale for error with this equation:

![cleveland equation](img/cleveland-equation.png)

You’ll be implementing this error score as part of the lab. 
(Hint: it’s not a trick question, this is just to familiarize you with the experiment protocol). 
With this Cleveland-McGill error score you can better compare the performance of the charts you test to figure out which one performs the best.

As a baseline, compare your average Error scores to the following chart, which include both Cleveland and McGill’s results as well as more recent extensions of this experiment (lower error indicates better performance, and error bars are bootstrapped 95% confidence intervals (`http://en.wikipedia.org/wiki/Confidence_interval#Meaning_and_interpretation`)):

![cleveland results](img/cleveland-results.png)

GitHub Details
---

- Fork the GitHub Repository. You now have a copy associated with your username.
- Make changes to index.html to fulfill the project requirements. 
- Make sure your "master" branch matches your "gh-pages" branch. See the GitHub Guides referenced above if you need help.
- Edit this README.md with a link to your gh-pages site: e.g. http://YourUsernameGoesHere.github.io/Experiment/index.html
- Replace this file (README.md) with your writeup and Design/Technical achievements.
- To submit, make a [Pull Request](https://help.github.com/articles/using-pull-requests/) on the original repository.

Papers, for Inspiration:
---
- Cleveland, W. S., & McGill, R. (1984). Graphical perception: Theory, experimentation, and application to the development of graphical methods. Journal of the American statistical association, 79(387), 531-554.
- Heer, J., & Bostock, M. (2010, April). Crowdsourcing graphical perception: using mechanical turk to assess visualization design. In Proceedings of the SIGCHI conference on human factors in computing systems (pp. 203-212). ACM.
- Talbot, J., Setlur, V., & Anand, A. (2014). Four experiments on the perception of bar charts. IEEE transactions on visualization and computer graphics, 20(12), 2152-2160.
- Kong, N., Heer, J., & Agrawala, M. (2010). Perceptual guidelines for creating rectangular treemaps. IEEE transactions on visualization and computer graphics, 16(6), 990-998.
- Skau, D., Harrison, L., & Kosara, R. (2015, June). An evaluation of the impact of visual embellishments in bar charts. In Computer Graphics Forum (Vol. 34, No. 3, pp. 221-230).
- Jansen, Y., & Hornbæk, K. (2016). A Psychophysical Investigation of Size as a Physical Variable. IEEE Trans. Vis. Comput. Graph., 22(1), 479-488.
- Kosara, R. (2016, October). An Empire Built On Sand: Reexamining What We Think We Know About Visualization. In Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization (pp. 162-168). ACM.
- Simkin, D., & Hastie, R. (1987). An information-processing analysis of graph perception. Journal of the American Statistical Association, 82(398), 454-465.
- Harrison, L., Skau, D., Franconeri, S., Lu, A., & Chang, R. (2013, April). Influencing visual judgment through affective priming. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 2949-2958). ACM.
- Kosara, R., & Ziemkiewicz, C. (2010, April). Do Mechanical Turks dream of square pie charts?. In Proceedings of the 3rd BELIV'10 Workshop: Beyond time and errors: Novel evaluation methods for information visualization (pp. 63-70). ACM.
- Boukhelifa, N., Bezerianos, A., Isenberg, T., & Fekete, J. D. (2012). Evaluating sketchiness as a visual variable for the depiction of qualitative uncertainty. IEEE Transactions on Visualization and Computer Graphics, 18(12), 2769-2778.
- Boy, J., Rensink, R. A., Bertini, E., & Fekete, J. D. (2014). A principled way of assessing visualization literacy. IEEE transactions on visualization and computer graphics, 20(12), 1963-1972.
- Moritz, D., Wang, C., Nelson, G. L., Lin, H., Smith, A. M., Howe, B., & Heer, J. (2019). Formalizing visualization design knowledge as constraints: Actionable and extensible models in Draco. IEEE transactions on visualization and computer graphics, 25(1), 438-448.
- Lee, S., Kim, S. H., & Kwon, B. C. (2017). Vlat: Development of a visualization literacy assessment test. IEEE transactions on visualization and computer graphics, 23(1), 551-560.

